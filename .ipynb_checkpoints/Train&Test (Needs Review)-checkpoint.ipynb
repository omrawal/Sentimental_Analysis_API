{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential \n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label        time                          date     query  \\\n",
       "1599994      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                username                                               text  \n",
       "1599994  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599995      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599996           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599997     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599998   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top results have the positive results while the bottom results have negative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'time', 'date', 'query', 'username', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of data is 1599999\n"
     ]
    }
   ],
   "source": [
    "print('lenght of data is', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall dataset is quite large. Therefore a portion of this data shall be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   label     1599999 non-null  int64 \n",
      " 1   time      1599999 non-null  int64 \n",
      " 2   date      1599999 non-null  object\n",
      " 3   query     1599999 non-null  object\n",
      " 4   username  1599999 non-null  object\n",
      " 5   text      1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no null values. Therefore, missing value treatment is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need the text and the label column. Therefore only those 2 columns are selected\n",
    "\n",
    "data=data[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values for label are 0 and 4. The 4 values are changed to 1\n",
    "\n",
    "data['label'][data['label']==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We seperate out the positive and negative samples and select 20000 from each. \n",
    "# The original dataset is too big. Therefore, having a smaller sample helps in increasing training speed\n",
    "# The original dataset is also imbalanced. Therefore, taking 20000 values from both hepls make the dataset balanced.\n",
    "# The number of samples can be increased to improve accuracy.\n",
    "\n",
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]\n",
    "\n",
    "data_pos = data_pos.iloc[:int(30000)]\n",
    "data_neg = data_neg.iloc[:int(30000)]\n",
    "\n",
    "# Merging the data\n",
    "\n",
    "data = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the text to lowercase\n",
    "\n",
    "data['text']=data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>oh ffs! i've been here all fucking day. why de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>leaving britney-just found out jon from new ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>@neomic i havta' go pee, but im scared to walk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>nooooooooooooooo!!!!!! school today. but the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>back to school tomorow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "29995  oh ffs! i've been here all fucking day. why de...      0\n",
       "29996  leaving britney-just found out jon from new ki...      0\n",
       "29997  @neomic i havta' go pee, but im scared to walk...      0\n",
       "29998  nooooooooooooooo!!!!!! school today. but the w...      0\n",
       "29999                            back to school tomorow       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omraw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data needs to be cleaned. \n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "#data['text'].head()\n",
    "\n",
    "\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "#data['text'].tail()\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "#data['text'].tail()\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_email(data):\n",
    "    return re.sub('@[^\\s]+', ' ', data)\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
    "#data['text'].tail()\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
    "#data['text'].tail()\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
    "#data['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>love healthuandpets u guys r best</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>im meting one besties tonight cant wait girl talk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>darealsunisakim thanks twiter ad sunisa got me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>sick realy cheap hurts much eat real fod plus ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>lovesbroklyn efect everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "799999                  love healthuandpets u guys r best      1\n",
       "800000  im meting one besties tonight cant wait girl talk      1\n",
       "800001  darealsunisakim thanks twiter ad sunisa got me...      1\n",
       "800002  sick realy cheap hurts much eat real fod plus ...      1\n",
       "800003                        lovesbroklyn efect everyone      1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999             [love, healthuandpets, u, guys, r, best]\n",
       "800000    [im, meting, one, besties, tonight, cant, wait...\n",
       "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800002    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800003                      [lovesbroklyn, efect, everyone]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>[love, healthuandpets, u, guys, r, best]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>[im, meting, one, besties, tonight, cant, wait...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>[darealsunisakim, thanks, twiter, ad, sunisa, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>[sick, realy, cheap, hurts, much, eat, real, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>[lovesbroklyn, efect, everyone]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "799999           [love, healthuandpets, u, guys, r, best]      1\n",
       "800000  [im, meting, one, besties, tonight, cant, wait...      1\n",
       "800001  [darealsunisakim, thanks, twiter, ad, sunisa, ...      1\n",
       "800002  [sick, realy, cheap, hurts, much, eat, real, f...      1\n",
       "800003                    [lovesbroklyn, efect, everyone]      1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: stemming_on_text(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>[love, healthuandpets, u, guys, r, best]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>[im, meting, one, besties, tonight, cant, wait...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>[darealsunisakim, thanks, twiter, ad, sunisa, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>[sick, realy, cheap, hurts, much, eat, real, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>[lovesbroklyn, efect, everyone]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "799999           [love, healthuandpets, u, guys, r, best]      1\n",
       "800000  [im, meting, one, besties, tonight, cant, wait...      1\n",
       "800001  [darealsunisakim, thanks, twiter, ad, sunisa, ...      1\n",
       "800002  [sick, realy, cheap, hurts, much, eat, real, f...      1\n",
       "800003                    [lovesbroklyn, efect, everyone]      1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "X=data['text']\n",
    "y=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "  l=[0,0]\n",
    "  temp=data.iloc[i,1]\n",
    "  l[temp-1]=1\n",
    "  y.append(l)\n",
    "\n",
    "print(y[0])\n",
    "print(y[-1])\n",
    "\n",
    "# [1,0] for positive emotion, [0,1] for negative emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "0\n",
      "466831\n",
      "7.780516666666666\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "min_len=1000\n",
    "sum_len=0\n",
    "avg_len=0\n",
    "\n",
    "\n",
    "for i in X:\n",
    "    if len(i)>max_len:\n",
    "        max_len=len(i)\n",
    "    if len(i)< min_len:\n",
    "        min_len=len(i)\n",
    "    sum_len=sum_len+len(i)\n",
    "avg_len=sum_len/len(X)\n",
    "print(max_len)\n",
    "print(min_len)\n",
    "print(sum_len)\n",
    "print(avg_len)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "num_words = 2000\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 100)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences_matrix, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(2000, 40, input_length=max_len))\n",
    "model.add(layers.LSTM(64))\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(128,activation='relu'))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 40)           80000     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                26880     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,802\n",
      "Trainable params: 164,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1500/1500 [==============================] - 97s 63ms/step - loss: 0.6925 - accuracy: 0.5154 - val_loss: 0.6896 - val_accuracy: 0.5557\n",
      "Epoch 2/40\n",
      "1500/1500 [==============================] - 94s 62ms/step - loss: 0.6602 - accuracy: 0.5919 - val_loss: 0.5813 - val_accuracy: 0.6865\n",
      "Epoch 3/40\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.5439 - accuracy: 0.7257 - val_loss: 0.5241 - val_accuracy: 0.7366\n",
      "Epoch 4/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.5104 - accuracy: 0.7511 - val_loss: 0.5143 - val_accuracy: 0.7457\n",
      "Epoch 5/40\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.4982 - accuracy: 0.7586 - val_loss: 0.5173 - val_accuracy: 0.7418\n",
      "Epoch 6/40\n",
      "1500/1500 [==============================] - 93s 62ms/step - loss: 0.4919 - accuracy: 0.7631 - val_loss: 0.5051 - val_accuracy: 0.7507\n",
      "Epoch 7/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.4878 - accuracy: 0.7675 - val_loss: 0.5031 - val_accuracy: 0.7542\n",
      "Epoch 8/40\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.4852 - accuracy: 0.7689 - val_loss: 0.5008 - val_accuracy: 0.7548\n",
      "Epoch 9/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.4832 - accuracy: 0.7688 - val_loss: 0.5080 - val_accuracy: 0.7484\n",
      "Epoch 10/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.4810 - accuracy: 0.7710 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
      "Epoch 11/40\n",
      "1500/1500 [==============================] - 85s 56ms/step - loss: 0.4798 - accuracy: 0.7715 - val_loss: 0.5049 - val_accuracy: 0.7498\n",
      "Epoch 12/40\n",
      "1500/1500 [==============================] - 84s 56ms/step - loss: 0.4782 - accuracy: 0.7706 - val_loss: 0.5011 - val_accuracy: 0.7533\n",
      "Epoch 13/40\n",
      "1500/1500 [==============================] - 84s 56ms/step - loss: 0.4766 - accuracy: 0.7723 - val_loss: 0.5031 - val_accuracy: 0.7527\n",
      "Epoch 14/40\n",
      "1500/1500 [==============================] - 84s 56ms/step - loss: 0.4826 - accuracy: 0.7692 - val_loss: 0.5058 - val_accuracy: 0.7515\n",
      "Epoch 15/40\n",
      "1500/1500 [==============================] - 84s 56ms/step - loss: 0.4773 - accuracy: 0.7722 - val_loss: 0.5007 - val_accuracy: 0.7519\n",
      "Epoch 16/40\n",
      "1500/1500 [==============================] - 92s 61ms/step - loss: 0.4738 - accuracy: 0.7741 - val_loss: 0.5058 - val_accuracy: 0.7545\n",
      "Epoch 17/40\n",
      "1500/1500 [==============================] - 147s 98ms/step - loss: 0.4720 - accuracy: 0.7746 - val_loss: 0.5043 - val_accuracy: 0.7550\n",
      "Epoch 18/40\n",
      "1500/1500 [==============================] - 122s 81ms/step - loss: 0.4693 - accuracy: 0.7763 - val_loss: 0.5014 - val_accuracy: 0.7553\n",
      "Epoch 19/40\n",
      "1500/1500 [==============================] - 95s 63ms/step - loss: 0.4676 - accuracy: 0.7772 - val_loss: 0.5096 - val_accuracy: 0.7521\n",
      "Epoch 20/40\n",
      "1500/1500 [==============================] - 92s 62ms/step - loss: 0.4644 - accuracy: 0.7783 - val_loss: 0.5090 - val_accuracy: 0.7478\n",
      "Epoch 21/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.4621 - accuracy: 0.7812 - val_loss: 0.5225 - val_accuracy: 0.7366\n",
      "Epoch 22/40\n",
      "1500/1500 [==============================] - 95s 63ms/step - loss: 0.4600 - accuracy: 0.7800 - val_loss: 0.5083 - val_accuracy: 0.7521\n",
      "Epoch 23/40\n",
      "1500/1500 [==============================] - 104s 69ms/step - loss: 0.4566 - accuracy: 0.7829 - val_loss: 0.5176 - val_accuracy: 0.7529\n",
      "Epoch 24/40\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.4540 - accuracy: 0.7840 - val_loss: 0.5075 - val_accuracy: 0.7525\n",
      "Epoch 25/40\n",
      "1500/1500 [==============================] - 112s 75ms/step - loss: 0.4505 - accuracy: 0.7855 - val_loss: 0.5154 - val_accuracy: 0.7523\n",
      "Epoch 26/40\n",
      "1500/1500 [==============================] - 165s 110ms/step - loss: 0.4479 - accuracy: 0.7867 - val_loss: 0.5136 - val_accuracy: 0.7469\n",
      "Epoch 27/40\n",
      "1500/1500 [==============================] - 135s 90ms/step - loss: 0.4455 - accuracy: 0.7881 - val_loss: 0.5247 - val_accuracy: 0.7405\n",
      "Epoch 28/40\n",
      "1500/1500 [==============================] - 143s 95ms/step - loss: 0.4415 - accuracy: 0.7905 - val_loss: 0.5139 - val_accuracy: 0.7464\n",
      "Epoch 29/40\n",
      "1500/1500 [==============================] - 160s 107ms/step - loss: 0.4385 - accuracy: 0.7923 - val_loss: 0.5361 - val_accuracy: 0.7513\n",
      "Epoch 30/40\n",
      "1500/1500 [==============================] - 142s 95ms/step - loss: 0.4351 - accuracy: 0.7940 - val_loss: 0.5161 - val_accuracy: 0.7460\n",
      "Epoch 31/40\n",
      "1500/1500 [==============================] - 153s 102ms/step - loss: 0.4314 - accuracy: 0.7959 - val_loss: 0.5395 - val_accuracy: 0.7443\n",
      "Epoch 32/40\n",
      "1500/1500 [==============================] - 115s 77ms/step - loss: 0.4282 - accuracy: 0.7974 - val_loss: 0.5380 - val_accuracy: 0.7483\n",
      "Epoch 33/40\n",
      "1500/1500 [==============================] - 94s 63ms/step - loss: 0.4259 - accuracy: 0.7990 - val_loss: 0.5266 - val_accuracy: 0.7467\n",
      "Epoch 34/40\n",
      "1500/1500 [==============================] - 97s 64ms/step - loss: 0.4209 - accuracy: 0.8014 - val_loss: 0.5498 - val_accuracy: 0.7460\n",
      "Epoch 35/40\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.4181 - accuracy: 0.8034 - val_loss: 0.5477 - val_accuracy: 0.7451\n",
      "Epoch 36/40\n",
      "1500/1500 [==============================] - 96s 64ms/step - loss: 0.4142 - accuracy: 0.8053 - val_loss: 0.5421 - val_accuracy: 0.7452\n",
      "Epoch 37/40\n",
      "1500/1500 [==============================] - 91s 61ms/step - loss: 0.4090 - accuracy: 0.8083 - val_loss: 0.5704 - val_accuracy: 0.7434\n",
      "Epoch 38/40\n",
      "1500/1500 [==============================] - 89s 59ms/step - loss: 0.4065 - accuracy: 0.8086 - val_loss: 0.5635 - val_accuracy: 0.7381\n",
      "Epoch 39/40\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.4021 - accuracy: 0.8123 - val_loss: 0.5624 - val_accuracy: 0.7429\n",
      "Epoch 40/40\n",
      "1500/1500 [==============================] - 86s 57ms/step - loss: 0.3989 - accuracy: 0.8134 - val_loss: 0.5706 - val_accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, y_train, epochs=40,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97888029e-01, 2.11195368e-03],\n",
       "       [2.86348024e-03, 9.97136474e-01],\n",
       "       [6.23350628e-02, 9.37664866e-01],\n",
       "       [4.21500087e-01, 5.78499913e-01],\n",
       "       [8.95941198e-01, 1.04058735e-01],\n",
       "       [2.55014966e-05, 9.99974489e-01],\n",
       "       [9.98445570e-01, 1.55449368e-03]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = ['This is the best day of my life', 'I feel like dying','I want to die I am a bad person','I feel like killing myself','I am happy','I am sad','I am very happy']\n",
    "\n",
    "sequences = tok.texts_to_sequences(list)\n",
    "testing_sequences = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "y_samples = np.array(testing_sequences)\n",
    "model.predict(y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  390,  355,  861,   93,    3,  315, 1345,\n",
       "         106],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  381,    5,\n",
       "         831],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,  381,   25,   56,  844,  381,  146,  952,   57,\n",
       "         461],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  381,    5,\n",
       "         721],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  381,\n",
       "         146],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  381,  146,\n",
       "          54],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  381,  146,\n",
       "        1432]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the best day of my life', 'I feel like dying', 'I want to die I am a bad person', 'I feel like killing myself', 'I am happy', 'I am sad', 'I am very happy']\n",
      "[[9.97888029e-01 2.11195368e-03]\n",
      " [2.86348024e-03 9.97136474e-01]\n",
      " [6.23350628e-02 9.37664866e-01]\n",
      " [4.21500087e-01 5.78499913e-01]\n",
      " [8.95941198e-01 1.04058735e-01]\n",
      " [2.55014966e-05 9.99974489e-01]\n",
      " [9.98445570e-01 1.55449368e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(list)\n",
    "print(model.predict(y_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save('model.h5', history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
